{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3016bb",
   "metadata": {},
   "source": [
    "## Name: Vishal Menon\n",
    "## Github username: vmeno0020\n",
    "## USC ID: 3377740336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6b68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaaaa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pelvic Incidence</th>\n",
       "      <th>Pelvic Tilt</th>\n",
       "      <th>Lumbar Lordosis Angle</th>\n",
       "      <th>Sacral Slope</th>\n",
       "      <th>Pelvic Radius</th>\n",
       "      <th>Grade of Spondylolisthesis</th>\n",
       "      <th>Class_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.03</td>\n",
       "      <td>22.55</td>\n",
       "      <td>39.61</td>\n",
       "      <td>40.48</td>\n",
       "      <td>98.67</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.06</td>\n",
       "      <td>10.06</td>\n",
       "      <td>25.02</td>\n",
       "      <td>29.00</td>\n",
       "      <td>114.41</td>\n",
       "      <td>4.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.83</td>\n",
       "      <td>22.22</td>\n",
       "      <td>50.09</td>\n",
       "      <td>46.61</td>\n",
       "      <td>105.99</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.30</td>\n",
       "      <td>24.65</td>\n",
       "      <td>44.31</td>\n",
       "      <td>44.64</td>\n",
       "      <td>101.87</td>\n",
       "      <td>11.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.71</td>\n",
       "      <td>9.65</td>\n",
       "      <td>28.32</td>\n",
       "      <td>40.06</td>\n",
       "      <td>108.17</td>\n",
       "      <td>7.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>47.90</td>\n",
       "      <td>13.62</td>\n",
       "      <td>36.00</td>\n",
       "      <td>34.29</td>\n",
       "      <td>117.45</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>53.94</td>\n",
       "      <td>20.72</td>\n",
       "      <td>29.22</td>\n",
       "      <td>33.22</td>\n",
       "      <td>114.37</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>61.45</td>\n",
       "      <td>22.69</td>\n",
       "      <td>46.17</td>\n",
       "      <td>38.75</td>\n",
       "      <td>125.67</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>45.25</td>\n",
       "      <td>8.69</td>\n",
       "      <td>41.58</td>\n",
       "      <td>36.56</td>\n",
       "      <td>118.55</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>33.84</td>\n",
       "      <td>5.07</td>\n",
       "      <td>36.64</td>\n",
       "      <td>28.77</td>\n",
       "      <td>123.95</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pelvic Incidence  Pelvic Tilt  Lumbar Lordosis Angle  Sacral Slope  \\\n",
       "0               63.03        22.55                  39.61         40.48   \n",
       "1               39.06        10.06                  25.02         29.00   \n",
       "2               68.83        22.22                  50.09         46.61   \n",
       "3               69.30        24.65                  44.31         44.64   \n",
       "4               49.71         9.65                  28.32         40.06   \n",
       "..                ...          ...                    ...           ...   \n",
       "305             47.90        13.62                  36.00         34.29   \n",
       "306             53.94        20.72                  29.22         33.22   \n",
       "307             61.45        22.69                  46.17         38.75   \n",
       "308             45.25         8.69                  41.58         36.56   \n",
       "309             33.84         5.07                  36.64         28.77   \n",
       "\n",
       "     Pelvic Radius  Grade of Spondylolisthesis  Class_Label  \n",
       "0            98.67                       -0.25            1  \n",
       "1           114.41                        4.56            1  \n",
       "2           105.99                       -3.53            1  \n",
       "3           101.87                       11.21            1  \n",
       "4           108.17                        7.92            1  \n",
       "..             ...                         ...          ...  \n",
       "305         117.45                       -4.25            0  \n",
       "306         114.37                       -0.42            0  \n",
       "307         125.67                       -2.71            0  \n",
       "308         118.55                        0.21            0  \n",
       "309         123.95                       -0.20            0  \n",
       "\n",
       "[310 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VertCol_df = pd.read_csv('../data/vertebral+column/column_2C.dat', sep=' ',header=None)\n",
    "VertCol_df.columns = ['Pelvic Incidence', 'Pelvic Tilt', 'Lumbar Lordosis Angle', 'Sacral Slope', 'Pelvic Radius', 'Grade of Spondylolisthesis', 'Class_Label']\n",
    "# assign Normal and Abnormal Class Labels as binary values: Normal NO = 0, Abnormal AB = 1\n",
    "VertCol_df.loc[VertCol_df.Class_Label == 'NO', 'Class_Label'] = 0\n",
    "VertCol_df.loc[VertCol_df.Class_Label == 'AB', 'Class_Label'] = 1\n",
    "VertCol_df.Class_Label = VertCol_df.Class_Label.astype(int)\n",
    "VertCol_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed84759",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2cc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(VertCol_df, hue='Class_Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Class_Label', y='Pelvic Incidence', data=VertCol_df, hue='Class_Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Class_Label', y='Pelvic Tilt', data=VertCol_df, hue='Class_Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Class_Label', y='Lumbar Lordosis Angle', data=VertCol_df, hue='Class_Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1664ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Class_Label', y='Sacral Slope', data=VertCol_df, hue='Class_Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Class_Label', y='Pelvic Radius', data=VertCol_df, hue='Class_Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Class_Label', y='Grade of Spondylolisthesis', data=VertCol_df, hue='Class_Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033e947",
   "metadata": {},
   "source": [
    "## Part B iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3118788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = VertCol_df[VertCol_df['Class_Label'] == 0]\n",
    "class_1 = VertCol_df[VertCol_df['Class_Label'] == 1]\n",
    "\n",
    "# Select 1st 70 rows of Class 0 and 1st 140 rows of Class 1 for training set\n",
    "train_class_0 = class_0.iloc[:70]\n",
    "train_class_1 = class_1.iloc[:140]\n",
    "\n",
    "# combine subsets for training set\n",
    "training_set = pd.concat([train_class_0, train_class_1])\n",
    "\n",
    "# remaining data for  test set\n",
    "test_class_0 = class_0.iloc[70:]\n",
    "test_class_1 = class_1.iloc[140:]\n",
    "\n",
    "# combine subsets to form the test set\n",
    "test_set = pd.concat([test_class_0, test_class_1])\n",
    "\n",
    "print(\"Training Set:\\n\", training_set)\n",
    "print(\"Testing Set:\\n\", test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead87531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing and training data\n",
    "X_train = training_set.drop('Class_Label', axis=1)\n",
    "y_train = training_set['Class_Label']\n",
    "X_test = test_set.drop('Class_Label', axis=1)\n",
    "y_test = test_set['Class_Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ae00b",
   "metadata": {},
   "source": [
    "## Part C i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7152a",
   "metadata": {},
   "source": [
    "## Part C ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(208, 0, -3)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for k in k_values:\n",
    "    KNN_EuclideanMetric = KNeighborsClassifier(n_neighbors=k, metric='euclidean') #euclidean metric classifier\n",
    "    KNN_EuclideanMetric.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on training and test data\n",
    "    train_pred = KNN_EuclideanMetric.predict(X_train)\n",
    "    test_pred = KNN_EuclideanMetric.predict(X_test)\n",
    "    \n",
    "    # Calculate and record errors\n",
    "    train_error = 1 - accuracy_score(y_train, train_pred)\n",
    "    test_error = 1 - accuracy_score(y_test, test_pred)\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(k_values, train_errors, label='Train Error')\n",
    "plt.plot(k_values, test_errors, label='Test Error')\n",
    "plt.gca().invert_xaxis()  # Reverse the x-axis to show decreasing values of k\n",
    "plt.xlabel('K Values')\n",
    "plt.ylabel('Error')\n",
    "plt.title('KNN Train vs Test Error For Different k Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ad60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most suitable k with minimum test error\n",
    "k_suitable = k_values[np.argmin(test_errors)]\n",
    "print('The most suitable k with the minimum test error is ', k_suitable)\n",
    "\n",
    "# use the suitable k* as 4 to train the model\n",
    "knn_ideal = KNeighborsClassifier(n_neighbors=k_suitable, metric='euclidean')\n",
    "knn_ideal.fit(X_train, y_train)\n",
    "suitable_prediction = knn_ideal.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fa805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_confusion_matrix = confusion_matrix(y_test, suitable_prediction)\n",
    "print('Confusion Matrix\\n', k_confusion_matrix)\n",
    "tn = k_confusion_matrix[0, 0]\n",
    "fp = k_confusion_matrix[0, 1]\n",
    "fn = k_confusion_matrix[1, 0]\n",
    "tp = k_confusion_matrix[1, 1]\n",
    "\n",
    "true_pos_rate = tp / (tp + fn)\n",
    "true_neg_rate = tn / (tn + fp)\n",
    "\n",
    "precision = precision_score(y_test, suitable_prediction)\n",
    "recall = recall_score(y_test, suitable_prediction)\n",
    "f1 = f1_score(y_test, suitable_prediction)\n",
    "\n",
    "print('True Positive Rate: ', true_pos_rate)\n",
    "print('True Negative Rate: ', true_neg_rate)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ccc02",
   "metadata": {},
   "source": [
    "## Part C iii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ed731",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_error_rates = [] # empty list initalized\n",
    "\n",
    "# Values of N\n",
    "N_values = range(10, 211, 20)  # Range of N values from 10 to 210 with a step of 20\n",
    "\n",
    "for N in N_values:\n",
    "    Class0_N = int(np.floor(N / 3))# subset of training set, 1st N/3 rows of Class 0\n",
    "    Class1_N = N - Class0_N # 1st N - (N/3) rows of Class 1\n",
    "    \n",
    "    if Class0_N > len(train_class_0) or Class1_N > len(train_class_1):\n",
    "        continue\n",
    "        \n",
    "    class0_subset = train_class_0.iloc[:Class0_N] # get the 1st N_class_0 rows from class0 df to form subset\n",
    "    class1_subset = train_class_1.iloc[:Class1_N] # get the 1st N_class_1 rows from class1 df to form subset\n",
    "    traindata_subset = pd.concat([class0_subset, class1_subset]) # combine class 0 and 1 subsets\n",
    "\n",
    "    \n",
    "    Xtrain_subset = traindata_subset.drop('Class_Label', axis=1) # split training data \n",
    "    Ytrain_subset = traindata_subset['Class_Label']\n",
    "\n",
    "    kval_range = range(1, N, 5) # range of k values for each N going in increments of 5\n",
    "\n",
    "    besttest_errorrate = float('inf') #set to infinity so any comparison with real error rates will result in smaller initial val \n",
    "    \n",
    "    for k in kval_range:\n",
    "        knn_new = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "        knn_new.fit(Xtrain_subset, Ytrain_subset)\n",
    "        \n",
    "        testset_prediction = knn_new.predict(X_test) # predict for test set\n",
    "        \n",
    "        errorrate = 1 - accuracy_score(y_test, testset_prediction) #error rate\n",
    "    \n",
    "        if errorrate < besttest_errorrate:\n",
    "            besttest_errorrate = errorrate\n",
    "    \n",
    "    best_error_rates.append(besttest_errorrate) #update error rate\n",
    "\n",
    "plt.figure(figsize=(8, 4)) # plot learning curve\n",
    "plt.plot(N_values, best_error_rates, color='blue', label='Test Error')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Best Test Error Rate for each N')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b5e5e",
   "metadata": {},
   "source": [
    "## Part D i. A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dceaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values_new = range(1, 197, 5) # k values {1,6,11,...,196}\n",
    "results = [] \n",
    "\n",
    "manhattan_error = [] # initialize list for errors\n",
    "for k in k_values_new:\n",
    "    KNNMinkowski = KNeighborsClassifier(n_neighbors = k, metric = 'minkowski', p = 1) # knn manhattan classifier when p = 1\n",
    "    KNNMinkowski.fit(X_train, y_train) #fit on training data\n",
    "    predicted_vals = KNNMinkowski.predict(X_test) # use test data to find predicted values\n",
    "    errorrate = 1 - accuracy_score(y_test, predicted_vals)\n",
    "    manhattan_error.append(errorrate)\n",
    "\n",
    "best_k_manhattan = k_values_new[np.argmin(manhattan_error)] # find the best k for manhattan metric by taking min of testing error\n",
    "best_manhattan_error = min(manhattan_error) \n",
    "results.append(('Manhattan', best_k_manhattan, 1, best_manhattan_error))\n",
    "errorresults_table = pd.DataFrame({      'Metric'       : 'Manhattan',\n",
    "                                         'k'            : k_values_new, \n",
    "                                         'Test Error'   : manhattan_error})\n",
    "print(errorresults_table)\n",
    "print('The best k value for Manhattan distance when p = 1 is ', best_k_manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8928e",
   "metadata": {},
   "source": [
    "## Part D i. B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kresults_log = []\n",
    "log_p = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "log_error = []\n",
    "for p in log_p:\n",
    "    KNN_log = KNeighborsClassifier(n_neighbors = 6, metric = 'minkowski', p=p)\n",
    "    KNN_log.fit(X_train, y_train)\n",
    "    predicted_vals = KNN_log.predict(X_test)\n",
    "    errorrate = 1 - accuracy_score(y_test, predicted_vals)\n",
    "    log_error.append(errorrate)\n",
    "    \n",
    "best_k_manhattan = log_p[np.argmin(log_error)]\n",
    "best_log_error = min(log_error)\n",
    "Kresults_log.append(('Manhattan', best_k_manhattan, p, errorrate))\n",
    "\n",
    "errorresults_table_log = pd.DataFrame({  'Metric'       : 'log10(p)',\n",
    "                                         'log10(p)'     :  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "                                         'Test Error'   : log_error})\n",
    "print(errorresults_table_log)\n",
    "    \n",
    "print('The best log10(p) is ', best_k_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17402c",
   "metadata": {},
   "source": [
    "## Part D i C) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values_new = range(1, 197, 5)\n",
    "#initialize empty list for results and error rate\n",
    "chebyshev_results = []\n",
    "chebyshev_error = []\n",
    "\n",
    "for k in k_values_new:\n",
    "    KNN_Chebyshev = KNeighborsClassifier(n_neighbors=k, metric='chebyshev') #chebyshev metric classifier\n",
    "    KNN_Chebyshev.fit(X_train, y_train) # fit on training data\n",
    "    predicted_values = KNN_Chebyshev.predict(X_test) # predicted values based on test data\n",
    "    errorrate = 1 - accuracy_score(y_test, predicted_values)\n",
    "    chebyshev_error.append(errorrate)\n",
    "\n",
    "best_k_chebyshev = k_values_new[np.argmin(chebyshev_error)] # find best k with min error from the range\n",
    "best_chebyshev_error = min(chebyshev_error)\n",
    "chebyshev_results.append(('Chebyshev', best_k_chebyshev, 'Infinity', best_chebyshev_error)) #add results to the list\n",
    "errorresults_table_chebyshev = pd.DataFrame({  'Metric'       : 'Chebyshev',\n",
    "                                         'k'            : k_values_new, \n",
    "                                         'Test Error'   : chebyshev_error})\n",
    "print(errorresults_table_chebyshev)\n",
    "print('The best k value for Chebyshev distance with p --> infinity is ', best_k_chebyshev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf885690",
   "metadata": {},
   "source": [
    "## Part D ii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52337b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_error = []\n",
    "\n",
    "cov_matrix = np.cov(X_train, rowvar=False)\n",
    "inverse_cov_matrix = inv(cov_matrix)\n",
    "\n",
    "# Define the range for k\n",
    "k_values_covariance = range(1, 197, 5)  # {1, 6, 11, ..., 196}\n",
    "\n",
    "best_mahalanobis_error = float('inf')\n",
    "best_k_mahalanobis = 0\n",
    "\n",
    "for k in k_values_covariance: \n",
    "    KNN_Mahalanobis = KNeighborsClassifier(n_neighbors=k, metric='mahalanobis', metric_params={'VI': inverse_cov_matrix}) # Mahalanobis classifier with metric param = inverse covariance matrix\n",
    "    KNN_Mahalanobis.fit(X_train, y_train) #fit on training data\n",
    "    predicted_values = KNN_Mahalanobis.predict(X_test)\n",
    "    errorrate = 1 - accuracy_score(y_test, predicted_values) #error rate\n",
    "    mahalanobis_error.append(errorrate)\n",
    "    \n",
    "    if errorrate < best_mahalanobis_error: #if encountered error rate is less than the best error rate, then set mahalanobis error to error rate\n",
    "        best_mahalanobis_error = errorrate\n",
    "        best_k_mahalanobis = k\n",
    "          \n",
    "errorresults_table_mahalanobis = pd.DataFrame({  'Metric'       : 'Mahalanobis',\n",
    "                                         'k'            : k_values_covariance, \n",
    "                                         'Test Error'   : mahalanobis_error})\n",
    "print(errorresults_table_mahalanobis)\n",
    "print('The best k value for Mahalanobis distance is ', best_k_mahalanobis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69393587",
   "metadata": {},
   "source": [
    "## Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 197, 5)\n",
    "\n",
    "best_error_euclidean = float('inf') # set to infinity so 1st error rate will be less than initial value\n",
    "best_k_euclidean = 0\n",
    "\n",
    "\n",
    "for k in k_values:#find Euclidean distance\n",
    "    KNN_Euclidean = KNeighborsClassifier(n_neighbors=k, metric='euclidean', weights='distance') # KNN Euclidean classifier with distance weight\n",
    "    KNN_Euclidean.fit(X_train, y_train)\n",
    "    predicted_vals = KNN_Euclidean.predict(X_test)\n",
    "    errorrate = 1 - accuracy_score(y_test, predicted_vals)\n",
    "    \n",
    "    if errorrate < best_error_euclidean: # current error rate should be best one for euclidean metric\n",
    "        best_error_euclidean = errorrate\n",
    "        best_k_euclidean = k\n",
    "\n",
    "print('Euclidean Distance\\nBest K: ', best_k_euclidean, '\\nBest Test Error:', best_error_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a28e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 197, 5)\n",
    "\n",
    "best_error_manhattan = float('inf') # set to infinity so 1st error rate will be less than initial value\n",
    "best_k_manhattan = 0\n",
    "\n",
    "# Manhattan distance\n",
    "for k in k_values:\n",
    "    KNN_W_Manhattan = KNeighborsClassifier(n_neighbors=k, metric='minkowski', weights='distance', p = 1) # KNN Manhattan classifier with distance weight\n",
    "    KNN_W_Manhattan.fit(X_train, y_train)\n",
    "    predicted_vals = KNN_W_Manhattan.predict(X_test)\n",
    "    errorrate = 1 - accuracy_score(y_test, predicted_vals)\n",
    "    \n",
    "    if errorrate < best_error_manhattan: # current error rate should be best 1 for manhattan metric\n",
    "        best_error_manhattan = errorrate\n",
    "        best_k_manhattan = k\n",
    "\n",
    "print('Manhattan Distance\\nBest K: ', best_k_manhattan, '\\nBest Test Error:', best_error_manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b56345",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 197, 5)\n",
    "\n",
    "best_error_chebyshev = float('inf') # set to infinity so 1st error rate will be less than initial value\n",
    "best_k_chebyshev = 0\n",
    "\n",
    "# chebyshev distance\n",
    "for k in k_values:\n",
    "    KNN_W_Chebyshev = KNeighborsClassifier(n_neighbors=k, metric='chebyshev', weights='distance', p = math.inf) # KNN Chebyshev classifier with distance weight\n",
    "    KNN_W_Chebyshev.fit(X_train, y_train)\n",
    "    predicted_vals = KNN_W_Chebyshev.predict(X_test)\n",
    "    errorrate = 1 - accuracy_score(y_test, predicted_vals)\n",
    "    \n",
    "    if errorrate < best_error_chebyshev: # current error rate should be best 1 for manhattan metric\n",
    "        best_error_chebyshev = errorrate\n",
    "        best_k_chebyshev = k\n",
    "\n",
    "print('Chebyshev Distance\\nBest K: ', best_k_chebyshev, '\\nBest Test Error:', best_error_chebyshev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343dd72",
   "metadata": {},
   "source": [
    "## Part F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29407b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_lowesteuclidean = min(train_errors)\n",
    "train_error_lowestmanhattan = min(manhattan_error)\n",
    "train_error_lowestchebyshev = min(chebyshev_error)\n",
    "print(train_error_lowesteuclidean, train_error_lowestmanhattan, train_error_lowestchebyshev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11393a6a",
   "metadata": {},
   "source": [
    "The lowest test error I achieved on this homework was 0.0 for Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e01ea",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a26cd",
   "metadata": {},
   "source": [
    "https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "\n",
    "https://realpython.com/knn-python/\n",
    "\n",
    "https://favtutor.com/blogs/infinity-python#:~:text=Python%20provides%20a%20predefined%20keyword,mathematical%20operations%20tends%20towards%20infinity.\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "https://stackoverflow.com/questions/62847769/how-to-find-all-the-true-positives-negatives-and-false-positives-and-negatives-c\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "\n",
    "https://stackoverflow.com/questions/48288047/scikit-learn-nearest-neighbor-search-with-weighted-distance-metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
